{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b93e3e9-f1b8-47de-b430-a83fddd8a35b",
   "metadata": {},
   "source": [
    "### Data Cleaning Reliability Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782c5193-3ff9-4bc9-9972-5707ba56cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb36766f-48fb-4608-abc6-409a39cd4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/complete_feat_dataset.csv\") \n",
    "\n",
    "\n",
    "df[\"step_count2\"] = df[\"act_walking_ep_0\"] + df[\"act_running_ep_0\"] # Create step_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "428a47d1-b3ff-439d-b78b-3e5d9accb1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # 35507 rows\n",
    "len(np.unique(df[\"uid\"])) # This also includes test IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91fd6fb9-2492-4349-80b3-ee581cae818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Index([], dtype='object', name='uid')\n",
      "9\n",
      "Daily_Cleaned_Dataset.csv created.\n"
     ]
    }
   ],
   "source": [
    "# Select Relevant Columns  (List of variables from Table 2 (based on preregistration)) \n",
    "variables_table2 = [ \n",
    "    # Sleep \n",
    "    'gm_sleep_duration',\"sleep_duration\", 'gm_sleep_duration_awake', \n",
    "    'gm_sleep_duration_deep', 'gm_sleep_duration_rem', 'gm_sleep_quality', \n",
    " \n",
    "    # Activity \n",
    "    'gm_dailies_step', 'step_count2', 'garmin_steps', \n",
    "    'act_still_ep_0', 'gm_dailies_active_kcal', 'gm_dailies_active_sec', \n",
    "    'gm_dailies_distance', 'gm_dailies_moderate_sec', \n",
    " \n",
    "    # Affective Dysregulation \n",
    "    'garmin_hrv_mean_ep_0', 'gm_dailies_activity_stress_duration', \n",
    "    'gm_dailies_average_stress', 'garmin_stress_mean_ep_0', \n",
    "    'gm_dailies_high_stress_duration', 'gm_dailies_low_stress_duration', \n",
    "    'gm_dailies_max_stress', 'gm_dailies_medium_stress_duration', \n",
    " \n",
    "    # Behavioral Inactivation \n",
    "    'unlock_duration_ep_0', 'unlock_num_ep_0', 'home_ep_0', \n",
    "    'loc_visit_num_ep_0', 'loc_dist_ep_0', \n",
    " \n",
    "    # Social Withdrawal \n",
    "    'audio_convo_duration_ep_0', 'audio_convo_num_ep_0', \n",
    "    'call_in_duration_ep_0', 'call_in_num_ep_0', \n",
    "    'call_out_duration_ep_0', 'call_out_num_ep_0', \n",
    "    'sms_in_num_ep_0', 'sms_out_num_ep_0' \n",
    "] \n",
    "\n",
    "id_cols = ['uid', 'day'] \n",
    "keep_cols = id_cols + variables_table2 \n",
    "df = df[[col for col in keep_cols if col in df.columns]] \n",
    "\n",
    "#df[['gm_sleep_duration']] = abs(8-df[['gm_sleep_duration']]) # I tried this as robustness check to account for too much or too little sleep, can be ignored\n",
    "#df[['sleep_duration']] = abs(8-df[['sleep_duration']])\n",
    "\n",
    "# Remove Implausible Dates \n",
    "df['day'] = pd.to_datetime(df['day'], errors='coerce') \n",
    "df = df[df['day'].dt.year > 2010]  # remove 1970 or other nvalid years \n",
    "print(len(np.unique(df[df['uid'].str.startswith('t')][\"uid\"])))# 10\n",
    "df = df[~df['uid'].str.startswith('t')] # remove test participants\n",
    "\n",
    "# Label Fully Zero Rows as Missing (not existing here) \n",
    "df['all_vars_zero'] = df.drop(columns=['day', 'uid']).eq(0).all(axis=1)\n",
    "df.loc[df['all_vars_zero'], variables_table2] = np.nan \n",
    "df.drop(columns='all_vars_zero', inplace=True) \n",
    "df[df[variables_table2].isna().all(axis=1)]\n",
    "\n",
    "# Remove Participants with All Missing Values (if all equals zero)\n",
    "df['row_missing'] = df[variables_table2].isna().all(axis=1) \n",
    "missing_per_participant = df.groupby('uid')['row_missing'].all() \n",
    "participants_to_drop = missing_per_participant[missing_per_participant].index \n",
    "print(participants_to_drop)\n",
    "df = df[~df['uid'].isin(participants_to_drop)] \n",
    "df.drop(columns='row_missing', inplace=True) \n",
    "df = df.reset_index()\n",
    "\n",
    "# Exclude participants who have less than 45 days of sensor data\n",
    "uid_counts = df.groupby(\"uid\").count()[\"day\"] \n",
    "uids_to_exclude = uid_counts[uid_counts < 45].index\n",
    "print(len(uids_to_exclude))\n",
    "df = df[~df['uid'].isin(uids_to_exclude)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Add numeric day, account for missing days, and account for weird breaks\n",
    "# Some participants have a certain start date but then pause for 2 weeks, Thus I exclude the first day and they need 3 days of data\n",
    "\n",
    "df[\"day\"] = pd.to_datetime(df.day, format=\"%Y-%m-%d\")\n",
    "\n",
    "uids = list(set(df[\"uid\"]))\n",
    "ESM_normalized = []\n",
    "\n",
    "for j in range(len(uids)):\n",
    "    ESM_part = df.loc[df[\"uid\"] == uids[j]]\n",
    "    ESM_part = ESM_part.reset_index(drop = True)\n",
    "\n",
    "    Dates = pd.DataFrame({\"day\": np.unique(ESM_part.day)})\n",
    "    time_range_date = pd.date_range(min(Dates.day),min(Dates.day) + timedelta(days=300), freq='D') # I just put a high number of dates here\n",
    "    time_range_date = pd.DataFrame({'day':time_range_date, 'uid':uids[j]})\n",
    "    ESM_part = pd.merge(ESM_part,time_range_date,how = 'right') \n",
    "    ESM_part = ESM_part.reset_index(drop = True)\n",
    "    \n",
    "    # Cut off first days if they are without data (three subsequent days need to have at least data\n",
    "    valid_rows = ESM_part[variables_table2].notna().all(axis=1)\n",
    "    rolling_valid = valid_rows.rolling(3).sum().shift(-2)\n",
    "    \n",
    "    # Find the first index where 3 consecutive rows have all valid data\n",
    "    start_index = rolling_valid[rolling_valid == 3].index.min()\n",
    "    if pd.notna(start_index):\n",
    "        ESM_part = ESM_part[ESM_part.index >= start_index].reset_index(drop=True)\n",
    "\n",
    "                                                       \n",
    "    Dates = pd.DataFrame({\"day\": np.unique(ESM_part.day)})\n",
    "    time_range_date = pd.date_range(min(Dates.day),min(Dates.day) + timedelta(days=90), freq='D') #create range 1 to 90\n",
    "    time_range_date = pd.DataFrame({'day':time_range_date, 'uid':uids[j]})\n",
    "    ESM_part = pd.merge(ESM_part,time_range_date,how = 'right') \n",
    "    ESM_part = ESM_part.reset_index(drop = True)\n",
    "\n",
    "    ESM_normalized.append(ESM_part)\n",
    "    \n",
    "df = pd.concat(ESM_normalized, ignore_index=True)\n",
    "df['day_num'] = df.groupby('uid').cumcount() + 1\n",
    "df = df.sort_values(by=['uid', 'day'])\n",
    "\n",
    "df = df[df['day_num'] <= 90] # Only use first 90 days (to avoid that some participants are overpresented)\n",
    "\n",
    "def reverse(df, reverse_items):\n",
    "\n",
    "    df_with_scores = df.copy()\n",
    "\n",
    "    # Automatically reverse code specified items using their min/max in the dataframe\n",
    "    for col in reverse_items:\n",
    "        if col in df_with_scores.columns:\n",
    "            col_min = df_with_scores[col].min()\n",
    "            col_max = df_with_scores[col].max()\n",
    "            df_with_scores[col] = (col_max + col_min) - df_with_scores[col]\n",
    "\n",
    "  \n",
    "    return df_with_scores\n",
    "\n",
    "  \n",
    "reverse_items = [\n",
    "    # Higher Sleep Disturbance (increase too much/little sleep, increased awake, increased rem, less deep, less sleep quality)\n",
    "    \"gm_sleep_quality\", \"gm_sleep_duration_deep\",\n",
    "\n",
    "    # Lower Activity (lower steps, increased still, lower kcal, lower activity sec, lower distance, lower moderate sec)\n",
    "    \"gm_dailies_step\", \"step_count2\", \"garmin_steps\",\n",
    "    \"gm_dailies_active_kcal\", \"gm_dailies_active_sec\", \n",
    "    \"gm_dailies_distance\", \"gm_dailies_moderate_sec\",\n",
    "\n",
    "    # Higher Affective Dysregulation (lower hrv, increased stress)\n",
    "    \"garmin_hrv_mean_ep_0\",\n",
    "\n",
    "    # Higher Behavioral Inactivation (higher unlock, higher home, lower locations)\n",
    "    \"loc_visit_num_ep_0\", \"loc_dist_ep_0\",\"unlock_num_ep_0\",\n",
    "\n",
    "    # Lower Social\n",
    "    'audio_convo_duration_ep_0', 'audio_convo_num_ep_0', \n",
    "    'call_in_duration_ep_0', 'call_in_num_ep_0', \n",
    "    'call_out_duration_ep_0', 'call_out_num_ep_0', \n",
    "    'sms_in_num_ep_0', 'sms_out_num_ep_0'\n",
    "]\n",
    "\n",
    "#reverse_items = []\n",
    "\n",
    "df = reverse(df, reverse_items)\n",
    "\n",
    "# Save Cleaned Dataset  \n",
    "df.to_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset.csv\", index=False) \n",
    "#df.to_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset_nonreverse.csv\", index=False) \n",
    "\n",
    "\n",
    "print(\"Daily_Cleaned_Dataset.csv created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78fdf14c-2c97-4d4c-8529-352f50ec9981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#313, 10 Test IDs\n",
    "#303, 9 had less than 45 days of data\n",
    "#294\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset.csv\")\n",
    "len(np.unique(df[\"uid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19ed44bf-2071-4b71-b942-9d8c4957d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly_Cleaned_Dataset.csv saved.\n",
      "Monthly_Cleaned_Dataset.csv saved.\n"
     ]
    }
   ],
   "source": [
    "### #Step 2 \n",
    "df = pd.read_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset.csv\") \n",
    "#df = pd.read_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset_nonreverse.csv\") \n",
    " \n",
    "df['day'] = pd.to_datetime(df['day']) \n",
    "df = df.sort_values(by=['uid', 'day']) \n",
    " \n",
    "# List of variables to aggregate (exclude id and date cols) \n",
    "exclude_cols = ['uid', 'day'] \n",
    "value_vars = [col for col in df.columns if col not in exclude_cols] \n",
    "\n",
    "### Weekly Aggregation ###\n",
    "# Create a participant-relative week index \n",
    "df['week_num'] = df.groupby('uid').cumcount() // 7 + 1\n",
    "\n",
    "# Group by participant and week, then compute the mean \n",
    "weekly_df = df.groupby(['uid', 'week_num'])[value_vars].mean().reset_index() \n",
    "weekly_df.to_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Weekly_Cleaned_Dataset.csv\", index=False) \n",
    "#weekly_df.to_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Weekly_Cleaned_Dataset_nonreverse.csv\", index=False) \n",
    "\n",
    "print(\"Weekly_Cleaned_Dataset.csv saved.\") \n",
    " \n",
    "### Monthly Aggregation ###\n",
    " \n",
    "# Create a participant-relative month index (each 30 days = 1 month) \n",
    "df['month_num'] = df.groupby('uid').cumcount() // 30 + 1\n",
    "\n",
    "# Group by participant and month, then compute the mean \n",
    "monthly_df = df.groupby(['uid', 'month_num'])[value_vars].mean().reset_index() \n",
    "monthly_df.to_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Monthly_Cleaned_Dataset.csv\", index=False) \n",
    "#monthly_df.to_csv(\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Monthly_Cleaned_Dataset_nonreverse.csv\", index=False) \n",
    "\n",
    "print(\"Monthly_Cleaned_Dataset.csv saved.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e807265e-c49e-445b-a487-17acdc3c5478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "### Step 3\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = \"minmax\"\n",
    "#scaler = \"standard\"\n",
    "\n",
    "datasets = {\n",
    "    \"Daily\": \"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset.csv\",\n",
    "    \"Weekly\": \"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Weekly_Cleaned_Dataset.csv\",\n",
    "    \"Monthly\": \"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Monthly_Cleaned_Dataset.csv\"\n",
    "}\n",
    "\n",
    "def apply_log_transform(df):\n",
    "    \n",
    "    return np.log1p(df)\n",
    "\n",
    "def apply_minmax_scaling(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "for name, filename in datasets.items():\n",
    "    df = pd.read_csv(filename)\n",
    "    #df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    #df = df.dropna()\n",
    "\n",
    "    # Log + Min-Max Scaling\n",
    "    log_transformed = df.copy()\n",
    "    log_transformed[variables_table2] = np.log1p(\n",
    "        log_transformed[variables_table2].clip(lower=0)  # avoid negatives\n",
    "    )\n",
    "    log_minmax_scaled = log_transformed.copy()\n",
    "    log_minmax_scaled[variables_table2] = apply_minmax_scaling(log_minmax_scaled[variables_table2])\n",
    "    log_minmax_scaled.to_csv(f\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/{name}_Cleaned_Dataset_Log_{scaler}.csv\", index=False)\n",
    "\n",
    "    # Min-Max Scaling Only\n",
    "    minmax_scaled_only = df.copy()\n",
    "    minmax_scaled_only[variables_table2] = apply_minmax_scaling(minmax_scaled_only[variables_table2])\n",
    "    minmax_scaled_only.to_csv(f\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/{name}_Cleaned_Dataset_{scaler}.csv\", index=False)\n",
    "\n",
    "print(\"Transformation complete. Files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8936335c-4859-4f92-8bb3-ce9b93a38b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset_Log_MinMax_Complete.csv\n",
      "Saved: /Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Daily_Cleaned_Dataset_MinMax_Complete.csv\n",
      "Saved: /Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Weekly_Cleaned_Dataset_Log_MinMax_Complete.csv\n",
      "Saved: /Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Weekly_Cleaned_Dataset_MinMax_Complete.csv\n",
      "Saved: /Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Monthly_Cleaned_Dataset_Log_MinMax_Complete.csv\n",
      "Saved: /Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/Monthly_Cleaned_Dataset_MinMax_Complete.csv\n"
     ]
    }
   ],
   "source": [
    "# Construct groups\n",
    "constructs = {\n",
    "    'Sleep_Score': [\n",
    "        'gm_sleep_duration', 'gm_sleep_duration_awake', \n",
    "        'gm_sleep_duration_deep', 'gm_sleep_duration_rem', \n",
    "        'gm_sleep_quality',\"sleep_duration\"\n",
    "    ],\n",
    "    'Activity_Score': [\n",
    "        'gm_dailies_step', 'step_count2', \n",
    "        'garmin_steps', 'act_still_ep_0', 'gm_dailies_active_kcal', \n",
    "        'gm_dailies_active_sec', 'gm_dailies_distance', \n",
    "        'gm_dailies_moderate_sec'\n",
    "    ],\n",
    "    'AffectiveDysregulation_Score': [\n",
    "        'garmin_hrv_mean_ep_0', 'gm_dailies_activity_stress_duration', \n",
    "        'gm_dailies_average_stress', 'garmin_stress_mean_ep_0', \n",
    "        'gm_dailies_high_stress_duration', 'gm_dailies_low_stress_duration', \n",
    "        'gm_dailies_max_stress', 'gm_dailies_medium_stress_duration'\n",
    "    ],\n",
    "    'BehavioralInactivation_Score': [\n",
    "        'unlock_duration_ep_0', 'unlock_num_ep_0', 'home_ep_0', \n",
    "        'loc_visit_num_ep_0', 'loc_dist_ep_0'\n",
    "    ],\n",
    "    'SocialWithdrawal_Score': [\n",
    "        'audio_convo_duration_ep_0', 'audio_convo_num_ep_0', \n",
    "        'call_in_duration_ep_0', 'call_in_num_ep_0', \n",
    "        'call_out_duration_ep_0', 'call_out_num_ep_0', \n",
    "        'sms_in_num_ep_0', 'sms_out_num_ep_0'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dataset base names\n",
    "dataset_types = ['Daily', 'Weekly', 'Monthly']\n",
    "transforms = ['Log_MinMax', 'MinMax']\n",
    "#transforms = ['Log_Standard', 'Standard']\n",
    "\n",
    "def add_construct_scores(df, constructs):\n",
    "    df_with_scores = df.copy()\n",
    "    # Recode reverse coded items\n",
    "    \n",
    "    # # Higher Sleep Disturbance (increase too much/little sleep, increased awake, increased rem, less deep, less sleep quality)\n",
    "    # df_with_scores[\"gm_sleep_quality\"] = 1 - df_with_scores[\"gm_sleep_quality\"]\n",
    "    # df_with_scores[\"gm_sleep_duration_deep\"] = 1 - df_with_scores[\"gm_sleep_duration_deep\"]\n",
    "    \n",
    "    # # Lower Activity (lower steps, increased still, lower kcal, lower activity sec, lower distance, lower moderate sec)\n",
    "    # df_with_scores[\"gm_dailies_step\"] = 1 - df_with_scores[\"gm_dailies_step\"]\n",
    "    # df_with_scores[\"step_count2\"] = 1 - df_with_scores[\"step_count2\"]\n",
    "    # df_with_scores[\"garmin_steps\"] = 1 - df_with_scores[\"garmin_steps\"]\n",
    "    # df_with_scores[\"gm_dailies_active_kcal\"] = 1 - df_with_scores[\"gm_dailies_active_kcal\"]\n",
    "    # df_with_scores[\"gm_dailies_active_sec\"] = 1 - df_with_scores[\"gm_dailies_active_sec\"]\n",
    "    # df_with_scores[\"gm_dailies_distance\"] = 1 - df_with_scores[\"gm_dailies_distance\"]\n",
    "    # df_with_scores[\"gm_dailies_moderate_sec\"] = 1 - df_with_scores[\"gm_dailies_moderate_sec\"]\n",
    "    # # double check what activity still means\n",
    "    \n",
    "    # # Higher Affective Dysregulation (lower hrv, increased stress)\n",
    "    # df_with_scores[\"garmin_hrv_mean_ep_0\"] = 1 - df_with_scores[\"garmin_hrv_mean_ep_0\"]\n",
    "    \n",
    "    # # Higher Behavioral Inactivation (higher unlock, higher home, lower locations)\n",
    "    # df_with_scores[\"loc_visit_num_ep_0\"] = 1 - df_with_scores[\"loc_visit_num_ep_0\"]\n",
    "    # df_with_scores[\"loc_dist_ep_0\"] = 1 - df_with_scores[\"loc_dist_ep_0\"]\n",
    "    # df_with_scores[\"unlock_num_ep_0\"] = 1 - df_with_scores[\"unlock_num_ep_0\"]\n",
    "    \n",
    "    # # Lower Social\n",
    "    # for col in [ 'audio_convo_duration_ep_0', 'audio_convo_num_ep_0', \n",
    "    #     'call_in_duration_ep_0', 'call_in_num_ep_0', \n",
    "    #     'call_out_duration_ep_0', 'call_out_num_ep_0', \n",
    "    #     'sms_in_num_ep_0', 'sms_out_num_ep_0'\n",
    "    # ]:\n",
    "    #     df_with_scores[col] = 1 - df_with_scores[col]\n",
    "\n",
    "\n",
    "    \n",
    "    for score_name, cols in constructs.items():\n",
    "        existing_cols = [col for col in cols if col in df.columns]\n",
    "        df_with_scores[score_name] = df[existing_cols].mean(axis=1, skipna=True)\n",
    "    return df_with_scores\n",
    "\n",
    "\n",
    "\n",
    "# Process and save updated datasets\n",
    "for dataset in dataset_types:\n",
    "    for transform in transforms:\n",
    "        filename = f\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/{dataset}_Cleaned_Dataset_{transform}.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "            df_with_scores = add_construct_scores(df, constructs)\n",
    "            output_name = f\"/Users/f007qrc/Library/CloudStorage/GoogleDrive-anna.m.langener@dartmouth.edu/My Drive/Darmouth Drive/4_Reliability Project_Data/{dataset}_Cleaned_Dataset_{transform}_Complete.csv\"\n",
    "            df_with_scores.to_csv(output_name, index=False)\n",
    "            print(f\"Saved: {output_name}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d043ef5-5315-48b7-b452-e0dd016883b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacbd1a-b4a3-4629-8def-9fb2619c6bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a8c62-6dd3-4037-97fe-79453d4370fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a2458-b795-4d57-b099-a7ef034b3daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
